{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "c3ab69c8",
   "metadata": {},
   "source": [
    "# Кредитное моделирование риска дефолта\n",
    "\n",
    "Классическая ML-задача бинарной классификации для оценки вероятности дефолта клиента на горизонте 12 месяцев. Ноутбук оформлен как реплицируемый конвейер: от загрузки данных до финального скоринга и интерпретации.\n",
    "\n",
    "## 1. Обзор проекта\n",
    "- **Цель:** предсказать вероятность дефолта клиента (PD) для поддержки кредитных решений.\n",
    "- **Таргет:** `flag` (0 — без дефолта, 1 — дефолт).\n",
    "- **Метрики:** ROC-AUC (основная), PR-AUC, F1/Recall (по бизнес-порогам).\n",
    "- **Результаты:** обученные артефакты модели, отчет о валидации, пайплайн инференса/скоринга."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "74caf0f5",
   "metadata": {},
   "outputs": [],
   "source": [
    "%pip install -q --upgrade pip setuptools wheel\n",
    "\n",
    "%pip install -q \\\n",
    "  \"numpy==1.26.4\" \\\n",
    "  \"pandas==2.2.2\" \\\n",
    "  \"scikit-learn==1.3.2\" \\\n",
    "  \"matplotlib==3.8.4\" \\\n",
    "  \"seaborn==0.13.2\" \\\n",
    "  \"ydata-profiling\" \\\n",
    "  \"xgboost==2.0.3\" \\\n",
    "  \"lightgbm==4.3.0\" \\\n",
    "  \"catboost==1.2.5\" \\\n",
    "  \"lightautoml\" \\\n",
    "  \"optuna==3.6.1\" \\\n",
    "  \"joblib>=1.3\" \\\n",
    "  \"tqdm>=4.66\" \\\n",
    "  \"ipywidgets==8.1.2\" \\\n",
    "  \"jupyterlab-widgets==3.0.10\" \\\n",
    "  \"nltk==3.9.1\" \\\n",
    "  \"gensim==4.3.3\" \\\n",
    "  \"transformers==4.44.2\" \\\n",
    "  \"sentencepiece>=0.1.99\" \\\n",
    "  \"tokenizers<0.20\"\\\n",
    "  \"pyarrow\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2e82b8ba",
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys, platform\n",
    "import pandas as pd, numpy as np, sklearn, matplotlib, seaborn as sns, ydata_profiling as ydata\n",
    "import xgboost, lightgbm, catboost\n",
    "import lightautoml as lama\n",
    "\n",
    "print(\"Python:\", sys.version.split()[0], \"|\", platform.platform())\n",
    "print(\"numpy:\", np.__version__)\n",
    "print(\"pandas:\", pd.__version__)\n",
    "print(\"sklearn:\", sklearn.__version__)\n",
    "print(\"matplotlib:\", matplotlib.__version__)\n",
    "print(\"seaborn:\", sns.__version__)\n",
    "print(\"xgboost:\", xgboost.__version__)\n",
    "print(\"lightgbm:\", lightgbm.__version__)\n",
    "print(\"catboost:\", catboost.__version__)\n",
    "print(\"lightautoml:\", lama.__version__)\n",
    "print(\"ydata_profiling:\", ydata.__version__)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "78bb0dee",
   "metadata": {},
   "source": [
    "## Содержание\n",
    "1. Обзор проекта\n",
    "2. Данные и словарь признаков\n",
    "3. Профилирование данных (ydata-profiling)\n",
    "4. Обработка пропусков и выбросов\n",
    "5. Генерация и отбор признаков (5 вариантов датасета)\n",
    "6. Разбиение данных: стратегии (для каждого варианта)\n",
    "7. Бейзлайн по датасетам\n",
    "8. AutoML\n",
    "9. Интерпретация и сдвиги данных\n",
    "10. Инференс и подготовка сабмита\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "45e92178",
   "metadata": {},
   "source": [
    "## 2. Данные и словарь признаков\n",
    "- **Файлы:**\n",
    "  - `train_data_*.pq` — табличные фичи по клиентам.\n",
    "  - `train_target.csv` — целевая переменная `flag`.\n",
    "  - `models/light_automl.pkl` — сохраненная модель.\n",
    "- **Идентификатор:** `id` (или иной уникальный ключ, при наличии).\n",
    "- **Словарь признаков:**\n",
    "  - Описать типы: числовые, категориальные, даты, агрегаты.\n",
    "  - Бизнес-смысл ключевых полей (доход, обороты, просрочки, лимиты и т.д.).\n",
    "- **Качество данных:**\n",
    "  - Доли пропусков, выбросы, сдвиги распределений между train/valid/test.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "2c1c247d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "            <div>\n",
       "                <ins><a href=\"https://ydata.ai/register\">Upgrade to ydata-sdk</a></ins>\n",
       "                <p>\n",
       "                    Improve your data and profiling with ydata-sdk, featuring data quality scoring, redundancy detection, outlier identification, text validation, and synthetic data generation.\n",
       "                </p>\n",
       "            </div>\n",
       "            "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import warnings\n",
    "import psutil, os, glob\n",
    "import pyarrow as pa\n",
    "import pyarrow.parquet as pq\n",
    "from tqdm import tqdm\n",
    "\n",
    "from lightautoml.automl.presets.tabular_presets import TabularAutoML, TabularUtilizedAutoML\n",
    "from lightautoml.tasks import Task\n",
    "import lightgbm as lgb\n",
    "from lightgbm import LGBMClassifier\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import seaborn as sns\n",
    "from xgboost import XGBClassifier\n",
    "from ydata_profiling import ProfileReport\n",
    "from ydata_profiling.utils.cache import cache_zipped_file\n",
    "\n",
    "\n",
    "from sklearn.compose import ColumnTransformer\n",
    "from sklearn.ensemble import (\n",
    "    AdaBoostClassifier,\n",
    "    ExtraTreesClassifier,\n",
    "    GradientBoostingClassifier,\n",
    "    IsolationForest,\n",
    "    RandomForestClassifier,\n",
    ")\n",
    "from sklearn.exceptions import DataConversionWarning\n",
    "from sklearn.impute import SimpleImputer\n",
    "from sklearn.linear_model import LogisticRegression, RidgeClassifier, SGDClassifier\n",
    "from sklearn.metrics import (\n",
    "    accuracy_score,\n",
    "    average_precision_score,\n",
    "    classification_report,\n",
    "    confusion_matrix,\n",
    "    log_loss,\n",
    "    roc_auc_score,\n",
    ")\n",
    "from sklearn.model_selection import (\n",
    "    GridSearchCV,\n",
    "    RandomizedSearchCV,\n",
    "    StratifiedKFold,\n",
    "    cross_validate,\n",
    "    train_test_split,\n",
    ")\n",
    "from sklearn.naive_bayes import GaussianNB\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.neural_network import MLPClassifier\n",
    "from sklearn.pipeline import make_pipeline\n",
    "from sklearn.preprocessing import OneHotEncoder, StandardScaler\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "\n",
    "\n",
    "warnings.filterwarnings(\"ignore\") \n",
    "\n",
    "pd.set_option(\"display.max_columns\", None)\n",
    "pd.set_option(\"display.max_rows\", None)\n",
    "pd.set_option(\"display.max_colwidth\", None)\n",
    "pd.options.display.float_format = '{:,.2f}'.format"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7b048a40",
   "metadata": {},
   "outputs": [],
   "source": [
    "PATH = \"train_data\"\n",
    "OUT_FILE = \"train_data_full.parquet\"\n",
    "\n",
    "files = sorted(\n",
    "    glob.glob(os.path.join(PATH, \"train_data_*.pq\")),\n",
    "    key=lambda p: int(os.path.splitext(os.path.basename(p))[0].split(\"_\")[-1])\n",
    ")\n",
    "\n",
    "mins, maxs, is_intlike = {}, {}, {}\n",
    "\n",
    "for f in tqdm(files, desc=\"Pass1: scan stats\"):\n",
    "    df = pd.read_parquet(f)\n",
    "    for c in df.columns:\n",
    "        s = df[c]\n",
    "\n",
    "        if s.dropna().empty:\n",
    "            continue\n",
    "        vmin, vmax = s.min(skipna=True), s.max(skipna=True)\n",
    "        mins[c] = vmin if c not in mins else min(mins[c], vmin)\n",
    "        maxs[c] = vmax if c not in maxs else max(maxs[c], vmax)\n",
    "\n",
    "        if c not in is_intlike:\n",
    "            is_intlike[c] = True\n",
    "        if pd.api.types.is_float_dtype(s):\n",
    "            frac = (s.dropna() - s.dropna().round()).abs().max()\n",
    "            if pd.notna(frac) and frac > 1e-9:\n",
    "                is_intlike[c] = False\n",
    "\n",
    "def pick_nullable_dtype(vmin, vmax, intlike=True):\n",
    "    if not intlike:\n",
    "        return None  \n",
    "    if vmin >= 0:\n",
    "        if vmax <= 1:\n",
    "            return \"boolean\"      \n",
    "        elif vmax <= 255:\n",
    "            return \"UInt8\"\n",
    "        elif vmax <= 65535:\n",
    "            return \"UInt16\"\n",
    "        elif vmax <= 4294967295:\n",
    "            return \"UInt32\"\n",
    "        else:\n",
    "            return \"UInt64\"\n",
    "    else:\n",
    "        if -128 <= vmin and vmax <= 127:\n",
    "            return \"Int8\"\n",
    "        elif -32768 <= vmin and vmax <= 32767:\n",
    "            return \"Int16\"\n",
    "        elif -2147483648 <= vmin and vmax <= 2147483647:\n",
    "            return \"Int32\"\n",
    "        else:\n",
    "            return \"Int64\"\n",
    "\n",
    "target_dtypes = {}\n",
    "for c in maxs.keys(): \n",
    "    dt = pick_nullable_dtype(mins.get(c, 0), maxs[c], is_intlike.get(c, False))\n",
    "    if dt is not None:\n",
    "        target_dtypes[c] = dt\n",
    "\n",
    "\n",
    "writer = None\n",
    "cols = None\n",
    "\n",
    "def cast_series(s: pd.Series, dtype: str) -> pd.Series:\n",
    "    if pd.api.types.is_float_dtype(s):\n",
    "        s = s.where(s.isna(), s.round())\n",
    "    return s.astype(dtype)\n",
    "\n",
    "try:\n",
    "    for i, f in enumerate(tqdm(files, desc=\"Pass2: write\")):\n",
    "        df = pd.read_parquet(f)\n",
    "\n",
    "        if cols is None:\n",
    "            cols = list(df.columns)\n",
    "\n",
    "        for c, dt in target_dtypes.items():\n",
    "            if c in df.columns:\n",
    "                df[c] = cast_series(df[c], dt)\n",
    "\n",
    "        df = df.reindex(columns=cols)\n",
    "\n",
    "        table = pa.Table.from_pandas(df, preserve_index=False)\n",
    "        if writer is None:\n",
    "            writer = pq.ParquetWriter(OUT_FILE, table.schema, compression=\"zstd\")\n",
    "        writer.write_table(table)\n",
    "\n",
    "        del df, table\n",
    "finally:\n",
    "    if writer is not None:\n",
    "        writer.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "6f65abaf",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_parquet(\"train_data_full.parquet\", engine=\"pyarrow\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "ad0af230",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total GB: 33.93 Used GB: 15.92 Avail GB: 18.01\n",
      "Proc RSS GB: 5.4\n"
     ]
    }
   ],
   "source": [
    "vm = psutil.virtual_memory()\n",
    "print(\"Total GB:\", round(vm.total/1e9,2), \"Used GB:\", round((vm.total-vm.available)/1e9,2), \"Avail GB:\", round(vm.available/1e9,2))\n",
    "\n",
    "proc = psutil.Process(os.getpid())\n",
    "print(\"Proc RSS GB:\", round(proc.memory_info().rss/1e9,2))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "14069857",
   "metadata": {},
   "source": [
    "## 3. Профилирование данных (ydata-profiling)\n",
    "- Генерация полного отчёта профилирования для `train_data_*.pq` + `train_target.csv`.\n",
    "- Блоки отчёта: overview, missingness, distributions, correlations, interactions, warnings.\n",
    "- Фиксация ключевых находок: дисбаланс классов, проблемные фичи, потенциальные утечки.\n",
    "- Ссылки/экспорт: HTML-отчёт, краткое резюме выводов.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bf37a95c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# profile = ProfileReport(\n",
    "#     df, title=\"Profile Report of the CreditRisk\", explorative=True\n",
    "# )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "66d4d60d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# profile"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c7e62641",
   "metadata": {},
   "source": [
    "## 4. Обработка пропусков и выбросов\n",
    "- Стратегии для числовых признаков: медиана/спец-коды/модели восстановления.\n",
    "- Для категориальных: частотная замена/`Unknown`.\n",
    "- Обработка выбросов: винсоризация/клиппинг/лог-трансформы.\n",
    "- Фиксация принятых правил, чтобы использовать их на инференсе.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c6b837e1",
   "metadata": {},
   "source": [
    "## 5. Генерация и отбор признаков: варианты датасетов\n",
    "- Базовые преобразования: агрегаты, лог-/норм-трансформы, масштабирование.\n",
    "- Кодирование категорий: Target/WOE/Count/Frequency, One-Hot (по необходимости).\n",
    "- Отбор: важности, устойчивость, мультиколлинеарность, бизнес-логика.\n",
    "- Ниже — 5 подвариантов подготовки датасета (будут использоваться далее для разных стратегий разбиения и бейзлайна).\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f2cfb50f",
   "metadata": {},
   "source": [
    "### 5.1 Вариант датасета A\n",
    "- Описание преобразований и отборов для варианта A.\n",
    "- Список ключевых фичей и логика их формирования.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "ec465d6e",
   "metadata": {},
   "outputs": [],
   "source": [
    "paym_cols = sorted([c for c in df.columns if c.startswith(\"enc_paym_\")],\n",
    "                   key=lambda s: int(s.split(\"_\")[-1]))\n",
    "\n",
    "\n",
    "last3  = paym_cols[:3]\n",
    "last6  = paym_cols[:6]\n",
    "last12 = paym_cols[:12]\n",
    "\n",
    "# 1) средний статус за 12 мес (ниже — лучше, 0=вовремя)\n",
    "df[\"paym_mean_12m\"] = df[last12].mean(axis=1).astype(\"float32\")\n",
    "\n",
    "# 2) худший статус за 12 мес (max)\n",
    "df[\"paym_max_12m\"]  = df[last12].max(axis=1).astype(\"float32\")\n",
    "\n",
    "# 3) была ли хоть какая-то задержка в последние 3 мес\n",
    "df[\"paym_any_late_3m\"] = df[last3].gt(0).any(axis=1).astype(\"uint8\")\n",
    "\n",
    "# 4) была ли серьёзная задержка (например, 30+ дней) за 12 мес\n",
    "df[\"paym_any_30plus_12m\"] = df[last12].ge(2).any(axis=1).astype(\"uint8\")\n",
    "\n",
    "# 5) рекурентная «свежесть проблем»: недавнее важнее\n",
    "w = (0.9 ** np.arange(len(last12))).astype(\"float32\")  # 0..11\n",
    "M = df[last12].to_numpy(dtype=\"float32\")\n",
    "df[\"paym_rw_mean_12m\"] = (M * w).sum(axis=1) / w.sum()\n",
    "df[\"paym_rw_mean_12m\"] = df[\"paym_rw_mean_12m\"].astype(\"float32\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "1ee0467b",
   "metadata": {},
   "outputs": [],
   "source": [
    "agg_funcs = {\n",
    "    # уже были\n",
    "    \"rn\": \"count\",\n",
    "    \"pre_since_opened\": [\"min\", \"max\", \"mean\"],\n",
    "    \"pre_pterm\": \"mean\",\n",
    "    \"pre_till_pclose\": \"mean\",\n",
    "    \"pclose_flag\": \"max\",\n",
    "\n",
    "    \"pre_loans_credit_limit\": [\"sum\", \"mean\", \"max\"],\n",
    "    \"pre_loans_outstanding\": \"sum\",\n",
    "    \"pre_loans_next_pay_summ\": [\"sum\", \"mean\"],\n",
    "    \"pre_loans_credit_cost_rate\": [\"mean\", \"max\"],\n",
    "    \"pre_util\": [\"mean\", \"max\"],\n",
    "    \"pre_over2limit\": \"max\",\n",
    "\n",
    "    \"pre_loans_total_overdue\": [\"mean\", \"max\", \"sum\"],\n",
    "    \"pre_loans_max_overdue_sum\": \"max\",\n",
    "\n",
    "    \"pre_loans5\": \"sum\",\n",
    "    \"pre_loans530\": \"sum\",\n",
    "    \"pre_loans3060\": \"sum\",\n",
    "    \"pre_loans6090\": \"sum\",\n",
    "    \"pre_loans90\": \"sum\",\n",
    "\n",
    "    \"paym_mean_12m\": [\"mean\", \"max\"],\n",
    "    \"paym_max_12m\": \"max\",\n",
    "    \"paym_any_late_3m\": \"max\",\n",
    "    \"paym_any_30plus_12m\": \"max\",\n",
    "    \"paym_rw_mean_12m\": [\"mean\", \"max\"],\n",
    "\n",
    "    \"pre_since_confirmed\": [\"min\", \"max\", \"mean\"],\n",
    "    \"pre_fterm\": [\"min\", \"max\", \"mean\"],\n",
    "    \"pre_till_fclose\": [\"min\", \"max\", \"mean\"],\n",
    "\n",
    "    \"fclose_flag\": \"max\",\n",
    "\n",
    "    \"is_zero_loans5\": \"max\",\n",
    "    \"is_zero_loans530\": \"max\",\n",
    "    \"is_zero_loans3060\": \"max\",\n",
    "    \"is_zero_loans6090\": \"max\",\n",
    "    \"is_zero_loans90\": \"max\",\n",
    "\n",
    "    \"pre_maxover2limit\": \"max\",\n",
    "    \"is_zero_util\": [\"mean\", \"max\"],\n",
    "    \"is_zero_over2limit\": [\"mean\", \"max\"],\n",
    "    \"is_zero_maxover2limit\": [\"mean\", \"max\"],\n",
    "\n",
    "    # энкодинги платежей enc_paym_0..24\n",
    "    **{f\"enc_paym_{i}\": [\"sum\", \"mean\", \"max\"] for i in range(25)},\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e2ba7296",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_client = df.groupby(\"id\", observed=True, sort=False).agg(agg_funcs)\n",
    "df_client.columns = [\n",
    "    \"_\".join(str(p) for p in col if p not in (None, \"\"))\n",
    "    for col in df_client.columns\n",
    "    ]\n",
    "def vc_mode(s: pd.Series):\n",
    "    s = s.dropna()\n",
    "    return s.value_counts().index[0] if len(s) else None\n",
    "\n",
    "cat_cols_safe = [\n",
    "    \"enc_loans_account_holder_type\",\n",
    "    \"enc_loans_credit_type\",\n",
    "    \"enc_loans_account_cur\",\n",
    "]\n",
    "for c in cat_cols_safe:\n",
    "    if c in df.columns:\n",
    "        df_client[c + \"_mode\"] = df.groupby(\"id\")[c].apply(vc_mode).values\n",
    "df_client = df_client.reset_index()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7cc84304",
   "metadata": {},
   "outputs": [],
   "source": [
    "target = pd.read_csv(\"train_target.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8dc82b85",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_all = (\n",
    "    df_client.set_index('id')\n",
    "             .join(target.set_index('id'), how='inner', validate='one_to_one')\n",
    "             .reset_index()\n",
    ")\n",
    "df_all[\"has_overdue_30\"] = ((df_all.get(\"pre_loans530_sum\", 0) +\n",
    "                        df_all.get(\"pre_loans3060_sum\", 0) +\n",
    "                        df_all.get(\"pre_loans6090_sum\", 0) +\n",
    "                        df_all.get(\"pre_loans90_sum\", 0)) > 0).astype(\"uint8\")\n",
    "df_all[\"has_overdue_90\"] = (df_all.get(\"pre_loans90_sum\", 0) > 0).astype(\"uint8\")\n",
    "\n",
    "\n",
    "df_all[\"overdue_share\"] = df_all[\"pre_loans_total_overdue_mean\"] / (df_all[\"pre_loans_outstanding_sum\"] + 1e-6)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0c99618b",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_all.to_parquet(\n",
    "    \"prepared_data/test2.1.parquet\",\n",
    "    index=False,\n",
    "    engine=\"pyarrow\",\n",
    "    compression=\"zstd\" \n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "04ba7d4e",
   "metadata": {},
   "source": [
    "### 5.2 Вариант датасета B\n",
    "- Отличия от A, альтернативные кодирования/агрегаты.\n",
    "- Риск утечек и как он предотвращается.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d512263f",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "1be8870d",
   "metadata": {},
   "source": [
    "### 5.3 Вариант датасета C\n",
    "- Специализированные признаки (временные окна/отношения/нормализации).\n",
    "- Обоснование выбора.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ae8a52f8",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "ef960aca",
   "metadata": {},
   "source": [
    "### 5.4 Вариант датасета D\n",
    "- Комбинации фичей/интеракции, редукция измерений (при необходимости).\n",
    "- Критерии отбора.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "057a93d5",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "0e8e5217",
   "metadata": {},
   "source": [
    "### 5.5 Вариант датасета E\n",
    "- Смесь подходов A–D, компромисс по качеству/скорости.\n",
    "- Список исключённых признаков и причины.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1ccc3ee9",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "f74d5ee5",
   "metadata": {},
   "source": [
    "## 6. Разбиение данных: стратегии\n",
    "- Общая логика: повторяем для каждого варианта датасета (A–E).\n",
    "- Варианты:\n",
    "  - Простое: `train`/`val`/`test` (стратифицированно по `flag`).\n",
    "  - С семплированием: undersampling/oversampling/SMOTE на `train`.\n",
    "- Фиксировать сиды, сохранять индексы/идентификаторы разбиений.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a2671ca2",
   "metadata": {},
   "source": [
    "### 6.A Разбиение для датасета A\n",
    "- Вариант 1: `train/val/test` (стратификация).\n",
    "- Вариант 2: семплирование на `train` (указать метод и параметры).\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "edf4f641",
   "metadata": {},
   "source": [
    "### 6.B Разбиение для датасета B\n",
    "- Вариант 1: `train/val/test` (стратификация).\n",
    "- Вариант 2: семплирование на `train` (указать метод и параметры).\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "385e995f",
   "metadata": {},
   "source": [
    "### 6.C Разбиение для датасета C\n",
    "- Вариант 1: `train/val/test` (стратификация).\n",
    "- Вариант 2: семплирование на `train` (указать метод и параметры).\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "60e53c71",
   "metadata": {},
   "source": [
    "### 6.D Разбиение для датасета D\n",
    "- Вариант 1: `train/val/test` (стратификация).\n",
    "- Вариант 2: семплирование на `train` (указать метод и параметры).\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d14f92de",
   "metadata": {},
   "source": [
    "### 6.E Разбиение для датасета E\n",
    "- Вариант 1: `train/val/test` (стратификация).\n",
    "- Вариант 2: семплирование на `train` (указать метод и параметры).\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2c00e942",
   "metadata": {},
   "source": [
    "## 7. Бейзлайн по датасетам\n",
    "- Для каждого варианта (A–E) обучить простой бейзлайн.\n",
    "- Фиксировать метрики на `val`/`test`, сохранять конфигурации и сиды.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8ac86b65",
   "metadata": {},
   "source": [
    "### 7.A Бейзлайн для датасета A\n",
    "- Модель(и): Logistic Regression / LightGBM (минимальная настройка).\n",
    "- Метрики: ROC-AUC, PR-AUC, F1/Recall (при пороге).\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f7fc8577",
   "metadata": {},
   "source": [
    "### 7.B Бейзлайн для датасета B\n",
    "- Модель(и): Logistic Regression / LightGBM.\n",
    "- Метрики и сравнение с A.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aab56152",
   "metadata": {},
   "source": [
    "### 7.C Бейзлайн для датасета C\n",
    "- Модель(и): Logistic Regression / LightGBM.\n",
    "- Метрики и сравнение с A/B.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "68a63a7a",
   "metadata": {},
   "source": [
    "### 7.D Бейзлайн для датасета D\n",
    "- Модель(и): Logistic Regression / LightGBM.\n",
    "- Метрики и сравнение с предыдущими.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "527e7ca4",
   "metadata": {},
   "source": [
    "### 7.E Бейзлайн для датасета E\n",
    "- Модель(и): Logistic Regression / LightGBM.\n",
    "- Итоговая таблица сравнения по всем вариантам.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2ef4c7c1",
   "metadata": {},
   "source": [
    "## 8. AutoML\n",
    "- Запуск AutoML (например, LightAutoML/FLAML/AutoGluon) на выбранных вариантах датасетов.\n",
    "- Ограничения по времени/ресурсам, критерии ранней остановки.\n",
    "- Сводка лучших конфигураций и сравнение с бейзлайнами.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c598ad94",
   "metadata": {},
   "source": [
    "## 9. Интерпретация и сдвиги данных\n",
    "- Важности признаков: Gain/Split, Permutation Importance.\n",
    "- Локальные объяснения: SHAP/ICE для кейсов.\n",
    "- Сдвиги: PSI/дрифт фичей между train/valid/test.\n",
    "- Проверка справедливости (bias) при наличии чувствительных атрибутов.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b04b6514",
   "metadata": {},
   "source": [
    "## 10. Инференс и подготовка сабмита\n",
    "- Единый конвейер препроцессинга и модели для прод/оффлайн скоринга.\n",
    "- Формат предсказаний: `client_id`, `score` (PD) и, при необходимости, `class` по бизнес-порогу.\n",
    "- Экспорт сабмита/отчёта, контроль версий и дат.\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
